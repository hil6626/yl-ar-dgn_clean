#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—åˆ†æå™¨ - Log Analyzer
ç”¨äºåˆ†æ logs ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶ï¼Œæå–é”™è¯¯ã€å¼‚å¸¸å’Œç»Ÿè®¡ä¿¡æ¯

ç”¨æ³•:
    python log_analyzer.py --path ../logs --summary
    python log_analyzer.py --follow --path ../logs

ä½œè€…: AI å…¨æ ˆæŠ€æœ¯å‘˜
ç‰ˆæœ¬: 1.0
"""

import argparse
import json
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List

BASE_DIR = Path(__file__).parent.parent.parent
DEFAULT_LOG_DIR = BASE_DIR / 'logs'


class LogAnalyzer:
    ERROR_PATTERNS = [r'ERROR', r'Exception', r'Traceback']

    def __init__(self, log_dir: Path = None):
        self.log_dir = log_dir or DEFAULT_LOG_DIR

    def list_logs(self) -> List[Path]:
        if not self.log_dir.exists():
            return []
        return sorted([p for p in self.log_dir.glob('**/*.log')])

    def analyze_file(self, filepath: Path) -> Dict:
        data = {'file': str(filepath), 'errors': [], 'lines': 0}
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                for i, line in enumerate(f, 1):
                    data['lines'] += 1
                    for pat in self.ERROR_PATTERNS:
                        if re.search(pat, line):
                            data['errors'].append({'line': i, 'text': line.strip()})
        except Exception as e:
            data['error'] = str(e)
        return data

    def analyze_all(self) -> Dict:
        result = {'time': datetime.now().isoformat(), 'files': []}
        for f in self.list_logs():
            result['files'].append(self.analyze_file(f))
        return result

    def tail_follow(self, filepath: Path):
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                # go to end
                f.seek(0, 2)
                while True:
                    line = f.readline()
                    if not line:
                        time.sleep(0.5)
                        continue
                    print(line, end='')
        except KeyboardInterrupt:
            return


def main():
    parser = argparse.ArgumentParser(description='æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('--path', type=str, default=str(DEFAULT_LOG_DIR), help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--summary', action='store_true', help='ç”Ÿæˆæ‘˜è¦')
    parser.add_argument('--file', type=str, help='åˆ†æå•ä¸ªæ–‡ä»¶')
    parser.add_argument('--follow', action='store_true', help='tail -f æ–‡ä»¶')
    parser.add_argument('--json', action='store_true', help='JSON è¾“å‡º')
    args = parser.parse_args()

    la = LogAnalyzer(Path(args.path))

    if args.file:
        res = la.analyze_file(Path(args.file))
    elif args.summary:
        res = la.analyze_all()
    elif args.follow and args.file:
        la.tail_follow(Path(args.file))
        return
    else:
        res = {'available_logs': [str(p) for p in la.list_logs()]}

    if args.json:
        print(json.dumps(res, ensure_ascii=False, indent=2))
    else:
        print(res)


if __name__ == '__main__':
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—åˆ†æè„šæœ¬ - Log Analyzer
ç”¨äºåˆ†æç³»ç»Ÿæ—¥å¿—ï¼Œæ£€æµ‹é”™è¯¯ã€è­¦å‘Šå’Œå¼‚å¸¸æ¨¡å¼

åŠŸèƒ½:
- åˆ†ææ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯å’Œè­¦å‘Š
- ç»Ÿè®¡æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
- æ£€æµ‹å¼‚å¸¸æ¨¡å¼
- ç”Ÿæˆåˆ†ææŠ¥å‘Š
- æ”¯æŒå®æ—¶æ—¥å¿—ç›‘æ§

ä½¿ç”¨æ–¹æ³•:
    python log_analyzer.py                              # åˆ†æé»˜è®¤æ—¥å¿—
    python log_analyzer.py --log-file /path/to/log      # åˆ†ææŒ‡å®šæ—¥å¿—
    python log_analyzer.py --analyze                     # è¯¦ç»†åˆ†ææ¨¡å¼
    python log_analyzer.py --watch                       # å®æ—¶ç›‘æ§æ¨¡å¼

ä½œè€…: AI å…¨æ ˆæŠ€æœ¯å‘˜
ç‰ˆæœ¬: 1.0
åˆ›å»ºæ—¥æœŸ: 2026å¹´1æœˆ30æ—¥
"""

import argparse
import json
import logging
import os
import re
import sys
import time
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path

# é…ç½®æ—¥å¿—
LOG_DIR = Path(__file__).parent.parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / "log_analyzer.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨ç±»"""
    
    # æ—¥å¿—çº§åˆ«æ­£åˆ™è¡¨è¾¾å¼
    LOG_LEVELS = {
        'ERROR': re.compile(r'\bERROR\b', re.IGNORECASE),
        'WARNING': re.compile(r'\bWARNING\b', re.IGNORECASE),
        'INFO': re.compile(r'\bINFO\b', re.IGNORECASE),
        'DEBUG': re.compile(r'\bDEBUG\b', re.IGNORECASE),
        'CRITICAL': re.compile(r'\bCRITICAL\b', re.IGNORECASE)
    }
    
    # å¸¸è§é”™è¯¯æ¨¡å¼
    ERROR_PATTERNS = [
        re.compile(r'Exception|Error|Failed|Timeout', re.IGNORECASE),
        re.compile(r'Traceback.*', re.IGNORECASE | re.DOTALL),
        re.compile(r'Connection.*refused', re.IGNORECASE),
        re.compile(r'FileNotFoundError', re.IGNORECASE),
        re.compile(r'PermissionError', re.IGNORECASE),
        re.compile(r'MemoryError', re.IGNORECASE),
        re.compile(r'IndexError|KeyError|TypeError|ValueError', re.IGNORECASE)
    ]
    
    def __init__(self, log_file=None, verbose=False):
        """
        åˆå§‹åŒ–æ—¥å¿—åˆ†æå™¨
        
        Args:
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.log_file = log_file
        self.verbose = verbose
        
        # è·¯å¾„é…ç½®
        self.base_dir = Path(__file__).parent.parent.parent
        self.logs_dir = self.base_dir / "logs"
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ—¥å¿—
        if self.log_file is None:
            self.log_file = self.logs_dir / "app.log"
    
    def read_logs(self, max_lines=10000):
        """
        è¯»å–æ—¥å¿—æ–‡ä»¶
        
        Args:
            max_lines: æœ€å¤§è¯»å–è¡Œæ•°
            
        Returns:
            list: æ—¥å¿—è¡Œåˆ—è¡¨
        """
        logs = []
        
        try:
            if isinstance(self.log_file, Path) or os.path.exists(self.log_file):
                with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    # è¯»å–æœ€å max_lines è¡Œ
                    lines = f.readlines()
                    logs = lines[-max_lines:] if len(lines) > max_lines else lines
            else:
                logger.warning(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.log_file}")
                # è¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
                logs = self._generate_sample_logs(100)
        except Exception as e:
            logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
            logs = self._generate_sample_logs(50)
        
        return logs
    
    def _generate_sample_logs(self, count=100):
        """ç”Ÿæˆç¤ºä¾‹æ—¥å¿—ç”¨äºæµ‹è¯•"""
        sample_logs = []
        levels = ['INFO', 'INFO', 'INFO', 'WARNING', 'ERROR']
        sources = ['monitor_app', 'api_handler', 'database', 'auth', 'scheduler']
        
        for i in range(count):
            level = levels[i % len(levels)]
            source = sources[i % len(sources)]
            timestamp = datetime.now() - timedelta(minutes=count - i)
            
            if level == 'ERROR':
                messages = [
                    f"API request failed: Connection timeout",
                    f"Database query error: {i}",
                    f"Service unavailable: {source}"
                ]
            elif level == 'WARNING':
                messages = [
                    f"High memory usage detected: {50 + i % 30}%",
                    f"Rate limit approaching for {source}",
                    f"Deprecated API call in {source}"
                ]
            else:
                messages = [
                    f"Request processed successfully",
                    f"Health check completed",
                    f"User action logged",
                    f"Scheduled task executed"
                ]
            
            log_line = f"{timestamp.isoformat()} [{level}] {source}: {messages[i % len(messages)]}"
            sample_logs.append(log_line)
        
        return sample_logs
    
    def parse_log_line(self, line):
        """
        è§£æå•è¡Œæ—¥å¿—
        
        Args:
            line: æ—¥å¿—è¡Œ
            
        Returns:
            dict: è§£æåçš„æ—¥å¿—ä¿¡æ¯
        """
        log_entry = {
            'raw': line.strip(),
            'timestamp': None,
            'level': 'UNKNOWN',
            'source': 'unknown',
            'message': line.strip(),
            'has_error_pattern': False
        }
        
        try:
            # è§£ææ—¶é—´æˆ³ (å¸¸è§æ ¼å¼: 2026-01-30 10:00:00)
            timestamp_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})')
            timestamp_match = timestamp_pattern.search(line)
            if timestamp_match:
                try:
                    log_entry['timestamp'] = datetime.fromisoformat(timestamp_match.group(1))
                except:
                    pass
            
            # è§£ææ—¥å¿—çº§åˆ«
            for level, pattern in self.LOG_LEVELS.items():
                if pattern.search(line):
                    log_entry['level'] = level
                    break
            
            # è§£ææ¥æº (æ ¼å¼: [source] æˆ– source:)
            source_pattern = re.compile(r'\[(\w+)\]|(\w+):')
            source_match = source_pattern.search(line)
            if source_match:
                log_entry['source'] = source_match.group(1) or source_match.group(2)
            
            # æ£€æµ‹é”™è¯¯æ¨¡å¼
            for pattern in self.ERROR_PATTERNS:
                if pattern.search(line):
                    log_entry['has_error_pattern'] = True
                    break
            
            # æå–æ¶ˆæ¯å†…å®¹
            if log_entry['timestamp']:
                # å¦‚æœæœ‰æ—¶æˆ³ï¼Œå°è¯•æå–åé¢çš„æ¶ˆæ¯
                parts = line.split(']', 2)
                if len(parts) > 2:
                    log_entry['message'] = parts[2].strip()
            
        except Exception as e:
            if self.verbose:
                logger.debug(f"è§£ææ—¥å¿—è¡Œå¤±è´¥: {e}")
        
        return log_entry
    
    def analyze(self, logs=None):
        """
        åˆ†ææ—¥å¿—
        
        Args:
            logs: æ—¥å¿—è¡Œåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è¯»å–æ—¥å¿—æ–‡ä»¶
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        if logs is None:
            logs = self.read_logs()
        
        if not logs:
            return {
                'status': 'empty',
                'message': 'æ²¡æœ‰æ—¥å¿—å¯åˆ†æ',
                'total_lines': 0
            }
        
        # è§£ææ‰€æœ‰æ—¥å¿—è¡Œ
        parsed_logs = [self.parse_log_line(line) for line in logs]
        
        # ç»Ÿè®¡åˆ†æ
        level_counts = Counter(log['level'] for log in parsed_logs)
        source_counts = Counter(log['source'] for log in parsed_logs)
        
        # é”™è¯¯æ—¥å¿—åˆ†æ
        error_logs = [log for log in parsed_logs if log['level'] in ['ERROR', 'CRITICAL'] or log['has_error_pattern']]
        warning_logs = [log for log in parsed_logs if log['level'] == 'WARNING']
        
        # æ—¶é—´åˆ†å¸ƒåˆ†æ
        time_distribution = defaultdict(int)
        for log in parsed_logs:
            if log['timestamp']:
                hour = log['timestamp'].hour
                time_distribution[hour] += 1
        
        # å¸¸è§é”™è¯¯æ¨¡å¼åˆ†æ
        error_patterns = Counter()
        for log in error_logs:
            # æå–é”™è¯¯ç±»å‹
            for pattern in self.ERROR_PATTERNS:
                match = pattern.search(log['message'])
                if match:
                    error_patterns[match.group(0).lower()] += 1
        
        # æœ€è¿‘çš„é”™è¯¯
        recent_errors = error_logs[:10] if error_logs else []
        
        # ç”Ÿæˆåˆ†æç»“æœ
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'analysis_period': {
                'start': parsed_logs[0]['timestamp'].isoformat() if parsed_logs and parsed_logs[0]['timestamp'] else None,
                'end': parsed_logs[-1]['timestamp'].isoformat() if parsed_logs and parsed_logs[-1]['timestamp'] else None
            },
            'statistics': {
                'total_lines': len(logs),
                'parsed_lines': len(parsed_logs),
                'level_distribution': dict(level_counts),
                'source_distribution': dict(source_counts),
                'error_count': len(error_logs),
                'warning_count': len(warning_logs),
                'error_rate': round(len(error_logs) / len(parsed_logs) * 100, 2) if parsed_logs else 0
            },
            'time_distribution': dict(time_distribution),
            'error_patterns': dict(error_patterns.most_common(10)),
            'recent_errors': [
                {
                    'timestamp': log['timestamp'].isoformat() if log['timestamp'] else None,
                    'level': log['level'],
                    'source': log['source'],
                    'message': log['message'][:200]
                }
                for log in recent_errors
            ],
            'recommendations': self._generate_recommendations(error_logs, level_counts)
        }
        
        return result
    
    def _generate_recommendations(self, error_logs, level_counts):
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        recommendations = []
        
        error_count = len(error_logs)
        
        if error_count == 0:
            recommendations.append("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæœªæ£€æµ‹åˆ°é”™è¯¯")
        
        if error_count > 10:
            recommendations.append("âš ï¸  é”™è¯¯æ•°é‡è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        
        if level_counts.get('WARNING', 0) > level_counts.get('ERROR', 0) * 2:
            recommendations.append("âš ï¸  è­¦å‘Šæ•°é‡è¾ƒé«˜ï¼Œå¯èƒ½å­˜åœ¨æ½œåœ¨é—®é¢˜")
        
        if error_count > 0:
            recommendations.append("ğŸ“‹ å»ºè®®æŸ¥çœ‹ recent_errors éƒ¨åˆ†äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        
        return recommendations
    
    def print_analysis(self, analysis_result):
        """æ§åˆ¶å°è¾“å‡ºåˆ†æç»“æœ"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ æ—¥å¿—åˆ†ææŠ¥å‘Š")
        print("=" * 70)
        
        if analysis_result['status'] == 'empty':
            print("æ²¡æœ‰æ—¥å¿—å¯åˆ†æ")
            return
        
        stats = analysis_result['statistics']
        
        print(f"\nâ° åˆ†ææ—¶é—´: {analysis_result['timestamp']}")
        print(f"ğŸ“Š æ€»æ—¥å¿—è¡Œæ•°: {stats['total_lines']}")
        print(f"   è§£ææˆåŠŸ: {stats['parsed_lines']}")
        
        print("\nğŸ“ˆ æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
        for level, count in sorted(stats['level_distribution'].items(), key=lambda x: -x[1]):
            bar = "â–ˆ" * (count // 10 + 1)
            print(f"   {level:8s}: {count:5d} {bar}")
        
        print("\nğŸ“Š é”™è¯¯ç‡: {:.2f}%".format(stats['error_rate']))
        
        if stats['error_count'] > 0:
            print(f"\nâŒ æ£€æµ‹åˆ° {stats['error_count']} æ¡é”™è¯¯")
            print("æœ€è¿‘é”™è¯¯:")
            for error in analysis_result['recent_errors'][:5]:
                timestamp = error['timestamp'].split('T')[1].split('.')[0] if error['timestamp'] else 'N/A'
                print(f"   [{timestamp}] {error['source']}: {error['message'][:80]}")
        
        if analysis_result['error_patterns']:
            print("\nğŸ” å¸¸è§é”™è¯¯æ¨¡å¼:")
            for pattern, count in list(analysis_result['error_patterns'].items())[:5]:
                print(f"   - {pattern}: {count} æ¬¡")
        
        print("\nğŸ’¡ å»ºè®®:")
        for rec in analysis_result['recommendations']:
            print(f"   {rec}")
        
        print("=" * 70 + "\n")
    
    def generate_report(self, output_path=None):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        """
        logs = self.read_logs()
        analysis_result = self.analyze(logs)
        
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = self.logs_dir / f"log_analysis_{timestamp}.json"
        
        report = {
            'report_type': 'Log Analysis Report',
            'generated_at': datetime.now().isoformat(),
            'analyzer_version': '1.0',
            'log_file': str(self.log_file),
            'analysis_result': analysis_result
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"æ—¥å¿—åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
            print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        
        return report
    
    def watch_logs(self, interval=5):
        """
        å®æ—¶ç›‘æ§æ—¥å¿—
        
        Args:
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        print(f"ğŸ‘€ å¼€å§‹å®æ—¶æ—¥å¿—ç›‘æ§: {self.log_file}")
        print("æŒ‰ Ctrl+C é€€å‡º\n")
        
        last_position = 0
        error_count = 0
        warning_count = 0
        
        try:
            while True:
                try:
                    if os.path.exists(self.log_file):
                        with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                            f.seek(last_position)
                            new_lines = f.readlines()
                            last_position = f.tell()
                            
                            for line in new_lines:
                                if 'ERROR' in line or 'CRITICAL' in line:
                                    error_count += 1
                                    print(f"âŒ [ERROR] {line.strip()}")
                                elif 'WARNING' in line:
                                    warning_count += 1
                                    print(f"âš ï¸  [WARNING] {line.strip()}")
                    
                    print(f"\rğŸ“Š ç›‘æ§ä¸­... é”™è¯¯: {error_count} | è­¦å‘Š: {warning_count}", end='', flush=True)
                    time.sleep(interval)
                except Exception as e:
                    logger.error(f"ç›‘æ§å‡ºé”™: {e}")
                    time.sleep(interval)
                    
        except KeyboardInterrupt:
            print(f"\n\nğŸ‘‹ ç›‘æ§å·²åœæ­¢ - æ€»é”™è¯¯: {error_count}, æ€»è­¦å‘Š: {warning_count}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ—¥å¿—åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    python log_analyzer.py                         # åˆ†æé»˜è®¤æ—¥å¿—
    python log_analyzer.py --log-file app.log      # åˆ†ææŒ‡å®šæ—¥å¿—
    python log_analyzer.py --analyze               # è¯¦ç»†åˆ†æå¹¶è¾“å‡º
    python log_analyzer.py --report                # ç”ŸæˆæŠ¥å‘Š
    python log_analyzer.py --watch                 # å®æ—¶ç›‘æ§
        """
    )
    
    parser.add_argument(
        '--log-file', '-f',
        type=str,
        default=None,
        help='æ—¥å¿—æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--analyze', '-a',
        action='store_true',
        help='è¯¦ç»†åˆ†ææ¨¡å¼'
    )
    
    parser.add_argument(
        '--report', '-r',
        action='store_true',
        help='ç”Ÿæˆåˆ†ææŠ¥å‘Š'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='æŠ¥å‘Šè¾“å‡ºè·¯å¾„'
    )
    
    parser.add_argument(
        '--watch', '-w',
        action='store_true',
        help='å®æ—¶ç›‘æ§æ¨¡å¼'
    )
    
    parser.add_argument(
        '--interval', '-i',
        type=int,
        default=5,
        help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = LogAnalyzer(log_file=args.log_file, verbose=args.verbose)
    
    try:
        if args.watch:
            # å®æ—¶ç›‘æ§æ¨¡å¼
            analyzer.watch_logs(args.interval)
        elif args.report:
            # ç”ŸæˆæŠ¥å‘Šæ¨¡å¼
            analyzer.generate_report(args.output)
        elif args.analyze:
            # è¯¦ç»†åˆ†ææ¨¡å¼
            logs = analyzer.read_logs()
            result = analyzer.analyze(logs)
            analyzer.print_analysis(result)
        else:
            # é»˜è®¤ï¼šç®€è¦åˆ†æå¹¶è¾“å‡ºJSON
            logs = analyzer.read_logs()
            result = analyzer.analyze(logs)
            print(json.dumps(result, ensure_ascii=False, indent=2))
    
    except Exception as e:
        logger.error(f"æ—¥å¿—åˆ†æå‡ºé”™: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()

