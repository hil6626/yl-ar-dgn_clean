#!/usr/bin/env python3
"""
Documentation Generator
YL-AR-DGN Documentation System
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import re


class DocsGenerator:
    """
    æ–‡æ¡£ç”Ÿæˆå™¨
    
    æä¾›æ–‡æ¡£ç”Ÿæˆã€éªŒè¯ã€é“¾æ¥æ£€æŸ¥å’Œæœç´¢ç´¢å¼•æ„å»ºåŠŸèƒ½ã€‚
    """
    
    def __init__(self, docs_root: str = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆå™¨
        
        Args:
            docs_root: æ–‡æ¡£æ ¹ç›®å½•
        """
        if docs_root is None:
            self.docs_root = Path(__file__).parent.parent / "docs"
        else:
            self.docs_root = Path(docs_root)
        
        self.output_dir = self.docs_root / "generated"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ–‡æ¡£å…ƒæ•°æ®
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½æ–‡æ¡£å…ƒæ•°æ®"""
        metadata_file = self.docs_root / "docs.json"
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"version": "1.0.0", "documents": {}}
    
    def save_metadata(self):
        """ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®"""
        metadata_file = self.docs_root / "docs.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
    
    def generate_index(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£ç´¢å¼•"""
        content = """# ğŸ“š YL-AR-DGN æ–‡æ¡£ä¸­å¿ƒ

**é¡¹ç›®:** YL-AR-DGN  
**ç‰ˆæœ¬:** {version}  
**æœ€åæ›´æ–°:** {generated}

---

## ğŸ“ æ–‡æ¡£ç›®å½•

### æ ¸å¿ƒæ–‡æ¡£

| æ–‡æ¡£ | æè¿° | çŠ¶æ€ |
|------|------|------|
| [README.md](README.md) | é¡¹ç›®ä¸»æ–‡æ¡£ | âœ… |
| [INDEX.md](INDEX.md) | æ–‡æ¡£å¯¼èˆª | âœ… |
| [TODO.md](TODO.md) | ä»»åŠ¡è¿›åº¦ | âœ… |
| [EXECUTION_RULES.md](EXECUTION_RULES.md) | æ‰§è¡Œè§„åˆ™ | âœ… |
| [DEPLOYMENT_SUMMARY.md](DEPLOYMENT_SUMMARY.md) | éƒ¨ç½²æ€»ç»“ | âœ… |

### é¡¹ç›®æ–‡æ¡£

| æ–‡æ¡£ | æè¿° | çŠ¶æ€ |
|------|------|------|
| [project/optimization-analysis.md](project/optimization-analysis.md) | ä¼˜åŒ–åˆ†æ | âœ… |
| [project/rules-docs/](project/rules-docs/) | è§„åˆ™æ–‡æ¡£ | âœ… |

### ä»»åŠ¡æ–‡æ¡£

| æ–‡æ¡£ | æè¿° | çŠ¶æ€ |
|------|------|------|
| [tasks/IMPLEMENTATION_SUMMARY.md](tasks/IMPLEMENTATION_SUMMARY.md) | æ‰§è¡Œæ€»ç»“ | âœ… |
| [tasks/TODO.md](tasks/TODO.md) | ä»»åŠ¡è¿›åº¦ | âœ… |

### éƒ¨ç½²æ–‡æ¡£

| æ–‡æ¡£ | æè¿° | çŠ¶æ€ |
|------|------|------|
| [DEPLOYMENT_SUMMARY.md](DEPLOYMENT_SUMMARY.md) | éƒ¨ç½²æ€»ç»“ | âœ… |
| [AR-backend/README_DEPLOYMENT.md](AR-backend/README_DEPLOYMENT.md) | åç«¯éƒ¨ç½² | âœ… |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–°æ‰‹å…¥é—¨

1. é˜…è¯» [README.md](README.md) äº†è§£é¡¹ç›®
2. æŸ¥çœ‹ [INDEX.md](INDEX.md) å¯¼èˆªæ–‡æ¡£
3. éµå¾ª [EXECUTION_RULES.md](EXECUTION_RULES.md) æ‰§è¡Œè§„åˆ™

### å¼€å‘æŒ‡å—

1. æŸ¥çœ‹ [project/optimization-analysis.md](project/optimization-analysis.md) äº†è§£æ¶æ„
2. å‚è€ƒæ¨¡å—æ–‡æ¡£è¿›è¡Œå¼€å‘
3. ä½¿ç”¨ Makefile è¿è¡Œå¸¸ç”¨å‘½ä»¤

### éƒ¨ç½²æŒ‡å—

1. é˜…è¯» [DEPLOYMENT_SUMMARY.md](DEPLOYMENT_SUMMARY.md)
2. é…ç½®ç¯å¢ƒå˜é‡
3. è¿è¡Œéƒ¨ç½²è„šæœ¬

---

## ğŸ“Š æ–‡æ¡£ç»Ÿè®¡

- **æ€»æ–‡æ¡£æ•°:** {total_docs}
- **å·²éªŒè¯:** {valid_docs}
- **å¾…æ›´æ–°:** {outdated_docs}

---

*æœ€åæ›´æ–°: {generated}*
""".format(
            version=self.metadata.get("version", "1.0.0"),
            generated=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            total_docs=len(self.metadata.get("documents", {})),
            valid_docs=sum(1 for d in self.metadata.get("documents", {}).values() if d.get("status") == "verified"),
            outdated_docs=sum(1 for d in self.metadata.get("documents", {}).values() if d.get("status") == "outdated")
        )
        
        return content
    
    def generate_toc(self, doc_path: Path) -> List[Dict[str, str]]:
        """ç”Ÿæˆæ–‡æ¡£ç›®å½•"""
        if not doc_path.exists():
            return []
        
        with open(doc_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£ææ ‡é¢˜ç”ŸæˆTOC
        lines = content.split('\n')
        toc = []
        for line in lines:
            if line.startswith('#'):
                level = len(line.split()[0])
                title = line.strip('#').strip()
                if title:
                    anchor = self._make_anchor(title)
                    toc.append({
                        'level': str(level),
                        'title': title,
                        'anchor': anchor
                    })
        
        return toc
    
    def _make_anchor(self, title: str) -> str:
        """ç”Ÿæˆé”šç‚¹é“¾æ¥"""
        anchor = title.lower()
        anchor = re.sub(r'[^\w\s-]', '', anchor)
        anchor = anchor.replace(' ', '-')
        anchor = anchor.replace('/', '-')
        return anchor
    
    def check_links(self, doc_path: Path) -> List[Dict[str, str]]:
        """æ£€æŸ¥æ–‡æ¡£é“¾æ¥"""
        broken_links = []
        
        if not doc_path.exists():
            return [{"file": str(doc_path), "link": "", "error": "File not found"}]
        
        with open(doc_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥å†…éƒ¨é“¾æ¥
        links = re.findall(r'\[.*?\]\((.*?)\)', content)
        
        for link in links:
            # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œé”šç‚¹
            if link.startswith(('http://', 'https://', '#', 'mailto:', 'tel:')):
                continue
            
            # æ£€æŸ¥ç›¸å¯¹è·¯å¾„
            link_path = doc_path.parent / link
            if not link_path.exists():
                broken_links.append({
                    "file": str(doc_path),
                    "link": link,
                    "error": "File not found"
                })
        
        return broken_links
    
    def validate_docs(self) -> Dict[str, Any]:
        """éªŒè¯æ‰€æœ‰æ–‡æ¡£"""
        report = {
            "check_time": datetime.now().isoformat(),
            "total_docs": 0,
            "valid_docs": 0,
            "broken_links": 0,
            "missing_docs": 0,
            "outdated_docs": 0,
            "details": []
        }
        
        for doc_id, doc_info in self.metadata.get("documents", {}).items():
            doc_path = self.docs_root / doc_info.get("path", "")
            report["total_docs"] += 1
            
            if not doc_path.exists():
                report["missing_docs"] += 1
                report["details"].append({
                    "id": doc_id,
                    "status": "missing",
                    "path": doc_info.get("path", ""),
                    "title": doc_info.get("title", "")
                })
            else:
                # æ£€æŸ¥é“¾æ¥
                broken_links = self.check_links(doc_path)
                if broken_links:
                    report["broken_links"] += len(broken_links)
                    report["details"].append({
                        "id": doc_id,
                        "status": "broken_links",
                        "path": doc_info.get("path", ""),
                        "title": doc_info.get("title", ""),
                        "links": broken_links
                    })
                else:
                    report["valid_docs"] += 1
                    report["details"].append({
                        "id": doc_id,
                        "status": "valid",
                        "path": doc_info.get("path", ""),
                        "title": doc_info.get("title", "")
                    })
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                last_mod = datetime.fromtimestamp(os.path.getmtime(doc_path))
                last_update = doc_info.get("updated", "")
                if last_update:
                    try:
                        expected_update = datetime.strptime(last_update, "%Y-%m-%d")
                        if last_mod.date() > expected_update.date():
                            report["outdated_docs"] += 1
                            report["details"].append({
                                "id": doc_id,
                                "status": "outdated",
                                "path": doc_info.get("path", ""),
                                "title": doc_info.get("title", "")
                            })
                    except ValueError:
                        pass
        
        return report
    
    def build_search_index(self) -> Dict[str, Any]:
        """æ„å»ºæœç´¢ç´¢å¼•"""
        search_index = {
            "version": "1.0",
            "last_updated": datetime.now().isoformat(),
            "documents": []
        }
        
        for doc_id, doc_info in self.metadata.get("documents", {}).items():
            doc_path = self.docs_root / doc_info.get("path", "")
            if doc_path.exists():
                with open(doc_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–å…³é”®è¯
                words = set()
                for word in re.findall(r'\b[a-zA-Z]{4,}\b', content):
                    if word.lower() not in ('this', 'that', 'with', 'from', 'have', 'will', 'been', 'were', 'they', 'their'):
                        words.add(word.lower())
                
                # æå–æ ‡é¢˜
                titles = []
                for line in content.split('\n'):
                    if line.startswith('#'):
                        titles.append(line.strip('#').strip())
                
                search_index["documents"].append({
                    "id": doc_id,
                    "title": doc_info.get("title", ""),
                    "path": doc_info.get("path", ""),
                    "description": doc_info.get("description", ""),
                    "tags": doc_info.get("tags", []),
                    "titles": titles[:5],
                    "keywords": list(words)[:50]
                })
        
        return search_index
    
    def register_document(
        self,
        doc_id: str,
        path: str,
        title: str,
        description: str = "",
        category: str = "general",
        tags: List[str] = None
    ):
        """æ³¨å†Œæ–‡æ¡£"""
        self.metadata["documents"][doc_id] = {
            "path": path,
            "title": title,
            "description": description,
            "category": category,
            "tags": tags or [],
            "status": "registered",
            "updated": datetime.now().strftime("%Y-%m-%d")
        }
        self.save_metadata()
    
    def update_document_status(self, doc_id: str, status: str):
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
        if doc_id in self.metadata.get("documents", {}):
            self.metadata["documents"][doc_id]["status"] = status
            self.metadata["documents"][doc_id]["updated"] = datetime.now().strftime("%Y-%m-%d")
            self.save_metadata()
    
    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£"""
        # ç”Ÿæˆç´¢å¼•
        index_content = self.generate_index()
        index_path = self.output_dir / "index.md"
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        # ç”Ÿæˆæœç´¢ç´¢å¼•
        search_index = self.build_search_index()
        search_path = self.output_dir / "search-index.json"
        with open(search_path, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        validation = self.validate_docs()
        validation_path = self.output_dir / "validation-report.json"
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation, f, ensure_ascii=False, indent=2)
        
        print(f"æ–‡æ¡£ç”Ÿæˆå®Œæˆ:")
        print(f"  - ç´¢å¼•: {index_path}")
        print(f"  - æœç´¢ç´¢å¼•: {search_path}")
        print(f"  - éªŒè¯æŠ¥å‘Š: {validation_path}")
        
        return {
            "index": str(index_path),
            "search_index": str(search_path),
            "validation_report": str(validation_path)
        }


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description="YL-AR-DGN æ–‡æ¡£ç”Ÿæˆå™¨")
    parser.add_argument("--generate", action="store_true", help="ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯æ–‡æ¡£")
    parser.add_argument("--check-links", type=str, help="æ£€æŸ¥æŒ‡å®šæ–‡æ¡£çš„é“¾æ¥")
    parser.add_argument("--build-index", action="store_true", help="æ„å»ºæœç´¢ç´¢å¼•")
    parser.add_argument("--register", type=str, nargs=4, metavar=("ID", "PATH", "TITLE", "DESC"), help="æ³¨å†Œæ–‡æ¡£")
    parser.add_argument("--status", type=str, nargs=2, metavar=("ID", "STATUS"), help="æ›´æ–°æ–‡æ¡£çŠ¶æ€")
    
    args = parser.parse_args()
    
    generator = DocsGenerator()
    
    if args.generate:
        result = generator.generate_all()
        print(f"ç”Ÿæˆç»“æœ: {result}")
    elif args.validate:
        report = generator.validate_docs()
        print(json.dumps(report, ensure_ascii=False, indent=2))
    elif args.check_links:
        doc_path = Path(args.check_links)
        broken_links = generator.check_links(doc_path)
        print(json.dumps(broken_links, ensure_ascii=False, indent=2))
    elif args.build_index:
        search_index = generator.build_search_index()
        print(json.dumps(search_index, ensure_ascii=False, indent=2))
    elif args.register:
        doc_id, path, title, description = args.register
        generator.register_document(doc_id, path, title, description)
        print(f"æ–‡æ¡£å·²æ³¨å†Œ: {doc_id}")
    elif args.status:
        doc_id, status = args.status
        generator.update_document_status(doc_id, status)
        print(f"æ–‡æ¡£çŠ¶æ€å·²æ›´æ–°: {doc_id} -> {status}")


if __name__ == "__main__":
    main()
