#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿ï¼ˆæœ¬åœ°ï¼‰
AR ç»¼åˆå®æ—¶åˆæˆä¸ç›‘æ§ç³»ç»Ÿ

åŠŸèƒ½:
- æœ¬åœ°è‡ªåŠ¨æµ‹è¯•æ‰§è¡Œ
- æµ‹è¯•ç»“æœæŠ¥å‘Šï¼ˆæœ¬åœ°JSON/HTMLï¼‰
- æµ‹è¯•å¤±è´¥æœ¬åœ°å‘Šè­¦
- æµ‹è¯•è¦†ç›–ç‡ç»Ÿè®¡

æ³¨æ„: æœ¬æµæ°´çº¿ä¸“æ³¨äºæœ¬åœ°æµ‹è¯•ï¼Œä¸æ¶‰åŠGitHub/GitLabç­‰è¿œç¨‹ä»“åº“CI/CDé…ç½®

ä½œè€…: AI å…¨æ ˆæŠ€æœ¯å‘˜
ç‰ˆæœ¬: 1.0
åˆ›å»ºæ—¥æœŸ: 2026å¹´2æœˆ9æ—¥
"""

import unittest
import sys
import os
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import logging

from test_utils import find_project_root, add_project_paths

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TestAutomationPipeline:
    """
    è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿ç±»
    
    åŠŸèƒ½:
    - è‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    - ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    - å¤±è´¥å‘Šè­¦é€šçŸ¥
    - æµ‹è¯•è¦†ç›–ç‡ç»Ÿè®¡
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æµ‹è¯•æµæ°´çº¿
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = {
            'test_dir': 'test',
            'output_dir': 'reports',
            'coverage': True,
            'verbose': True,
            'fail_fast': False,
            'parallel': False,
            'max_workers': 4,
        }
        if config:
            self.config.update(config)
        
        # æµ‹è¯•ç»“æœ
        self.results = {
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'errors': 0,
            'skipped': 0,
            'duration': 0,
            'timestamp': None,
            'failures': [],
            'errors_list': [],
        }
        
        # é¡¹ç›®æ ¹ç›®å½•
        self.project_root = find_project_root(Path(__file__).resolve())
        add_project_paths(self.project_root)
        self.test_dir = self.project_root / self.config['test_dir']
        self.output_dir = self.project_root / self.config['output_dir']

        # è¦†ç›–ç‡ç›®æ ‡
        self.coverage_targets = [
            str(self.project_root / "AR-backend"),
        ]
        if not any(Path(target).exists() for target in self.coverage_targets):
            self.config['coverage'] = False
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def run_all_tests(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        
        Returns:
            Dict: æµ‹è¯•ç»“æœ
        """
        logger.info("å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿...")
        start_time = time.time()
        
        # æ”¶é›†æµ‹è¯•å¥—ä»¶
        test_suites = self._collect_test_suites()
        
        # è¿è¡Œæµ‹è¯•
        for suite_name, suite_path in test_suites.items():
            logger.info(f"è¿è¡Œæµ‹è¯•å¥—ä»¶: {suite_name}")
            suite_result = self._run_test_suite(suite_path, suite_name)
            self._merge_results(suite_result)
        
        # è®¡ç®—æ€»æ—¶é—´
        self.results['duration'] = time.time() - start_time
        self.results['timestamp'] = datetime.now().isoformat()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
        
        logger.info(f"æµ‹è¯•æµæ°´çº¿å®Œæˆ: {self.results['passed']}/{self.results['total_tests']} é€šè¿‡")
        
        return self.results
    
    def _collect_test_suites(self) -> Dict[str, Path]:
        """
        æ”¶é›†æµ‹è¯•å¥—ä»¶
        
        Returns:
            Dict[str, Path]: æµ‹è¯•å¥—ä»¶åç§°å’Œè·¯å¾„
        """
        test_suites = {}
        
        # å•å…ƒæµ‹è¯•
        unit_test_dir = self.test_dir / 'test_backend'
        if unit_test_dir.exists():
            for f in unit_test_dir.glob('test_*.py'):
                if f.is_file():
                    test_suites[f.stem] = f
        
        # é›†æˆæµ‹è¯•
        integration_test = self.test_dir / 'integration' / 'test_modules.py'
        if integration_test.exists():
            test_suites['integration'] = integration_test
        
        # æ€§èƒ½æµ‹è¯•
        performance_test = self.test_dir / 'performance' / 'test_performance.py'
        if performance_test.exists():
            test_suites['performance'] = performance_test
        
        # ç¨³å®šæ€§æµ‹è¯•
        stability_test = self.test_dir / 'stability' / 'test_stability.py'
        if stability_test.exists():
            test_suites['stability'] = stability_test
        
        return test_suites
    
    def _run_test_suite(self, test_path: Path, suite_name: str) -> Dict:
        """
        è¿è¡Œæµ‹è¯•å¥—ä»¶
        
        Args:
            test_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            suite_name: æµ‹è¯•å¥—ä»¶åç§°
            
        Returns:
            Dict: æµ‹è¯•ç»“æœ
        """
        result = {
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'errors': 0,
            'skipped': 0,
            'duration': 0,
            'failures': [],
            'errors_list': [],
        }
        
        start_time = time.time()
        
        # æ„å»ºpytestå‘½ä»¤
        json_report_path = self.output_dir / f'{suite_name}_result.json'

        cmd = [
            sys.executable, '-m', 'pytest',
            str(test_path),
            '-v', '--tb=short',
            '--json-report', '--json-report-file=' + str(json_report_path)
        ]
        
        if self.config['fail_fast']:
            cmd.append('-x')
        
        if self.config['coverage']:
            for target in self.coverage_targets:
                if Path(target).exists():
                    cmd.append(f'--cov={target}')
            cmd.append('--cov-report=term-missing')
        
        try:
            # æ‰§è¡Œæµ‹è¯•
            process = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # è§£æJSONæŠ¥å‘Šç»“æœ
            if json_report_path.exists():
                try:
                    with open(json_report_path, 'r', encoding='utf-8') as f:
                        report = json.load(f)
                    summary = report.get('summary', {})
                    result['total_tests'] = summary.get('total', 0)
                    result['passed'] = summary.get('passed', 0)
                    result['failed'] = summary.get('failed', 0)
                    result['errors'] = summary.get('errors', 0)
                    result['skipped'] = summary.get('skipped', 0)
                except Exception:
                    result['passed'] = self._count_tests(process.stdout, 'passed')
                    result['failed'] = self._count_tests(process.stdout, 'failed')
                    result['errors'] = self._count_tests(process.stdout, 'error')
            else:
                result['passed'] = self._count_tests(process.stdout, 'passed')
                result['failed'] = self._count_tests(process.stdout, 'failed')
                result['errors'] = self._count_tests(process.stdout, 'error')
            
            # ä¿å­˜è¾“å‡º
            log_file = self.output_dir / f'{suite_name}_output.log'
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(process.stdout)
                f.write(process.stderr)
            
        except subprocess.TimeoutExpired:
            result['errors'] = 1
            result['errors_list'].append(f'{suite_name}: æµ‹è¯•è¶…æ—¶')
        except Exception as e:
            result['errors'] = 1
            result['errors_list'].append(f'{suite_name}: {str(e)}')
        
        result['duration'] = time.time() - start_time
        
        return result
    
    def _count_tests(self, output: str, status: str) -> int:
        """
        è®¡ç®—æµ‹è¯•æ•°é‡
        
        Args:
            output: pytestè¾“å‡º
            status: çŠ¶æ€ (passed, failed, error)
            
        Returns:
            int: æµ‹è¯•æ•°é‡
        """
        try:
            # å°è¯•è§£æJSONæŠ¥å‘Š
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨pytestçš„JSONæŠ¥å‘Š
            for line in output.split('\n'):
                if f'{status}' in line.lower():
                    # æå–æ•°å­—
                    words = line.split()
                    for word in words:
                        if word.isdigit():
                            return int(word)
            return 0
        except Exception:
            return 0
    
    def _merge_results(self, suite_result: Dict) -> None:
        """
        åˆå¹¶æµ‹è¯•ç»“æœ
        
        Args:
            suite_result: æµ‹è¯•å¥—ä»¶ç»“æœ
        """
        self.results['total_tests'] += suite_result['total_tests']
        self.results['passed'] += suite_result['passed']
        self.results['failed'] += suite_result['failed']
        self.results['errors'] += suite_result['errors']
        self.results['skipped'] += suite_result['skipped']
        self.results['duration'] += suite_result['duration']
        self.results['failures'].extend(suite_result.get('failures', []))
        self.results['errors_list'].extend(suite_result.get('errors_list', []))
    
    def _generate_report(self) -> None:
        """
        ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        """
        report = {
            'summary': {
                'total_tests': self.results['total_tests'],
                'passed': self.results['passed'],
                'failed': self.results['failed'],
                'errors': self.results['errors'],
                'skipped': self.results['skipped'],
                'pass_rate': self._calculate_pass_rate(),
                'duration_seconds': round(self.results['duration'], 2),
                'timestamp': self.results['timestamp'],
            },
            'failures': self.results['failures'],
            'errors': self.results['errors_list'],
            'config': self.config,
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = self.output_dir / 'test_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(report)
        
        logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def _generate_html_report(self, report: Dict) -> None:
        """
        ç”ŸæˆHTMLæŠ¥å‘Š
        
        Args:
            report: æµ‹è¯•æŠ¥å‘Šæ•°æ®
        """
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR ç»¼åˆå®æ—¶åˆæˆä¸ç›‘æ§ç³»ç»Ÿ - æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        .summary {{ background: #f5f5f5; padding: 20px; border-radius: 5px; margin: 20px 0; }}
        .pass {{ color: green; }}
        .fail {{ color: red; }}
        .error {{ color: orange; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 10px; text-align: left; }}
        th {{ background: #4CAF50; color: white; }}
        .duration {{ color: #666; }}
    </style>
</head>
<body>
    <h1>ğŸ¯ AR ç»¼åˆå®æ—¶åˆæˆä¸ç›‘æ§ç³»ç»Ÿ - æµ‹è¯•æŠ¥å‘Š</h1>
    
    <div class="summary">
        <h2>ğŸ“Š æµ‹è¯•æ‘˜è¦</h2>
        <p>æ‰§è¡Œæ—¶é—´: {report['summary']['timestamp']}</p>
        <p>æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}</p>
        <p class="pass">âœ… é€šè¿‡: {report['summary']['passed']}</p>
        <p class="fail">âŒ å¤±è´¥: {report['summary']['failed']}</p>
        <p class="error">âš ï¸ é”™è¯¯: {report['summary']['errors']}</p>
        <p>é€šè¿‡ç‡: {report['summary']['pass_rate']:.2f}%</p>
        <p class="duration">æ€»è€—æ—¶: {report['summary']['duration_seconds']}ç§’</p>
    </div>
    
    <h2>ğŸ“‹ å¤±è´¥è¯¦æƒ…</h2>
    {self._generate_failure_html(report['failures'])}
    
    <h2>ğŸ“‹ é”™è¯¯è¯¦æƒ…</h2>
    {self._generate_error_html(report['errors'])}
    
</body>
</html>
        """
        
        html_file = self.output_dir / 'test_report.html'
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html)
    
    def _generate_failure_html(self, failures: List) -> str:
        """ç”Ÿæˆå¤±è´¥è¯¦æƒ…HTML"""
        if not failures:
            return "<p>æ— å¤±è´¥è®°å½• âœ…</p>"
        
        html = "<table><tr><th>åºå·</th><th>è¯¦æƒ…</th></tr>"
        for i, failure in enumerate(failures, 1):
            html += f"<tr><td>{i}</td><td>{failure}</td></tr>"
        html += "</table>"
        return html
    
    def _generate_error_html(self, errors: List) -> str:
        """ç”Ÿæˆé”™è¯¯è¯¦æƒ…HTML"""
        if not errors:
            return "<p>æ— é”™è¯¯è®°å½• âœ…</p>"
        
        html = "<table><tr><th>åºå·</th><th>é”™è¯¯</th></tr>"
        for i, error in enumerate(errors, 1):
            html += f"<tr><td>{i}</td><td>{error}</td></tr>"
        html += "</table>"
        return html
    
    def _calculate_pass_rate(self) -> float:
        """è®¡ç®—é€šè¿‡ç‡"""
        total = self.results['total_tests']
        if total == 0:
            return 0
        return (self.results['passed'] / total) * 100
    
    def check_test_status(self) -> Dict:
        """
        æ£€æŸ¥æµ‹è¯•çŠ¶æ€
        
        Returns:
            Dict: æµ‹è¯•çŠ¶æ€
        """
        return {
            'status': 'healthy' if self.results['failed'] == 0 and self.results['errors'] == 0 else 'needs_attention',
            'total_tests': self.results['total_tests'],
            'passed': self.results['passed'],
            'failed': self.results['failed'],
            'errors': self.results['errors'],
            'pass_rate': self._calculate_pass_rate(),
        }


# GitHub Actions CI/CD é…ç½®æ–‡ä»¶
GITHUB_ACTIONS_WORKFLOW = """name: AR System Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist
    
    - name: Run unit tests
      run: |
        pytest test/test_backend/ -v --cov=src --cov-report=xml
    
    - name: Run integration tests
      run: |
        pytest test/integration/ -v
    
    - name: Run performance tests
      run: |
        pytest test/performance/ -v
    
    - name: Run stability tests
      run: |
        pytest test/stability/ -v
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
"""

# GitLab CI é…ç½®æ–‡ä»¶
GITLAB_CI_FILE = """stages:
  - test
  - report

test:
  stage: test
  script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    - pip install pytest pytest-cov
  artifacts:
    reports:
      junit: test-results.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

test:integration:
  stage: test
  script:
    - pip install pytest
    - pytest test/integration/ -v
  artifacts:
    reports:
      junit: test-integration-results.xml

test:performance:
  stage: test
  script:
    - pip install pytest
    - pytest test/performance/ -v

test:stability:
  stage: test
  script:
    - pip install pytest
    - pytest test/stability/ -v

pages:
  stage: report
  script:
    - echo "Generating test report..."
  artifacts:
    paths:
      - public
    expire_in: 1 week
"""


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæµ‹è¯•æµæ°´çº¿"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', action='store_true', help='ç”ŸæˆæŠ¥å‘Š')
    parser.add_argument('--ci', action='store_true', help='ç”ŸæˆCI/CDé…ç½®')
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•æµæ°´çº¿
    pipeline = TestAutomationPipeline()
    
    # è¿è¡Œæµ‹è¯•
    results = pipeline.run_all_tests()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ¯ æµ‹è¯•æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
    print("="*50)
    print(f"æ€»æµ‹è¯•æ•°: {results['total_tests']}")
    print(f"âœ… é€šè¿‡: {results['passed']}")
    print(f"âŒ å¤±è´¥: {results['failed']}")
    print(f"âš ï¸ é”™è¯¯: {results['errors']}")
    print(f"é€šè¿‡ç‡: {pipeline._calculate_pass_rate():.2f}%")
    print(f"æ€»è€—æ—¶: {results['duration']:.2f}ç§’")
    print("="*50)
    
    # ç”ŸæˆCI/CDé…ç½®
    if args.ci:
        # GitHub Actions
        with open('.github/workflows/tests.yml', 'w') as f:
            f.write(GITHUB_ACTIONS_WORKFLOW)
        print("\nâœ… GitHub Actions é…ç½®å·²ç”Ÿæˆ: .github/workflows/tests.yml")
        
        # GitLab CI
        with open('.gitlab-ci.yml', 'w') as f:
            f.write(GITLAB_CI_FILE)
        print("âœ… GitLab CI é…ç½®å·²ç”Ÿæˆ: .gitlab-ci.yml")


if __name__ == '__main__':
    main()
